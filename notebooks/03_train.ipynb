{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:12.076539Z",
     "start_time": "2020-06-15T16:06:12.047621Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from slice_generator import SliceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:13.284414Z",
     "start_time": "2020-06-15T16:06:13.277779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:15.516904Z",
     "start_time": "2020-06-15T16:06:15.505686Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = 1\n",
    "channels = 1\n",
    "pixels_x = 21\n",
    "pixels_y = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get and save the pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:17.551021Z",
     "start_time": "2020-06-15T16:06:17.527128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../models/stack_single_input/log.log',\n",
      " '../models/stack_single_input/ssi_hist1.pkl',\n",
      " '../models/stack_single_input/stack_single_input_1F.h5',\n",
      " '../models/stack_single_input/train_val_loss.png']\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../models/\"\n",
    "model_folder = \"stack_single_input/\"\n",
    "pwd = model_dir+model_folder\n",
    "file_list = sorted(glob.glob(pwd+\"*\"))\n",
    "pprint(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:17.789615Z",
     "start_time": "2020-06-15T16:06:17.784870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... \n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Loading done for stack_single_input_1F\n"
     ]
    }
   ],
   "source": [
    "# choose a model\n",
    "models=[]\n",
    "for file in file_list:\n",
    "    match = re.search(\"\\.h5$\", file)\n",
    "    if match:\n",
    "        models.append(file)\n",
    "\n",
    "assert len(models) > 0, \"Hmm, can't seem to find any model files ending in .h5 in \"+pwd\n",
    "if len(models) > 1:\n",
    "    print(\"There are more than one models in the directory.\\nLoading the first.\\n\", models)\n",
    "print(\"Loading... \")\n",
    "model = tf.keras.models.load_model(models[0])\n",
    "\n",
    "model_name = re.search(\"[^/]*(?=\\.[^.]+($|\\?))\", models[0]).__getitem__(0)\n",
    "print(\"Loading done for\",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name += \"_\"+loss_func_name\n",
    "# model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Learning Rate, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "Define a custom loss function. Here it's SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_name = \"SSE\"\n",
    "def sum_sq_err(y_true, y_pred):\n",
    "    return K.sum( K.square(y_true - y_pred), axis=1)\n",
    "class SumSquaredError(tf.keras.losses.Loss):\n",
    "    def __init__():\n",
    "        self.name = \"SSE\"\n",
    "    def call(self, y_true, y_pred):\n",
    "#         y_pred = ops.convert_to_tensor_v2(y_pred)\n",
    "#         y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "        return K.sum( K.square(y_true - y_pred), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr\n",
    "Define a Learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "# lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate = 1./3.1415,\n",
    "#     decay_steps = 1,\n",
    "#     decay_rate = 0.367,\n",
    "#     staircase=False,\n",
    "#     name=\"inversePi_e_inverse7\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer\n",
    "Define an optimizer. Must set ```clipnorm``` to prevent wild things from happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,\n",
    "    nesterov=False,\n",
    "    name='SGD',\n",
    "    clipnorm=1.0,\n",
    ")\n",
    "# tf.keras.optimizers.Adam(learning_rate=lr,\n",
    "#                                beta_1=0.9,\n",
    "#                                beta_2=0.999,\n",
    "#                                epsilon=1e-07,\n",
    "#                                amsgrad=False,\n",
    "#                                name='Adam',\n",
    "#                                clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling stack_single_input_1F ...\n",
      "Compiled.\n"
     ]
    }
   ],
   "source": [
    "print(\"Compiling\", model_name,\"...\")\n",
    "model.compile(\n",
    "    loss=sum_sq_err,\n",
    "    optimizer=opt,\n",
    "    metrics=['mean_absolute_error'])\n",
    "print(\"Compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for training, callbacks\n",
    "Get paths to training and validation sets, and number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:19.687719Z",
     "start_time": "2020-06-15T16:06:19.677223Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file_path = \"../data/train\"\n",
    "valid_file_path = \"../data/validate\"\n",
    "vars_           = ['t2m']\n",
    "proc_type       = \"convlstm\"\n",
    "# 3 years of training data = \n",
    "train_steps = 3 * 365 * 24 / frames\n",
    "# 1 year of validation data = \n",
    "valid_steps = 1 * 365 * 24 / frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks\n",
    "Define callbacks to periodically save the model, stop early, write logs to tensorboard, and stream loss metrics to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=pwd+model_name+\".h5\", \n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='auto',\n",
    "        save_freq='epoch',\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=100,\n",
    "        # \"no longer improving\" being further defined as \"for at least 1 epochs\"\n",
    "        patience=1,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        filename=pwd+\"log.log\",\n",
    "        separator=',',\n",
    "        append=True\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:20.382027Z",
     "start_time": "2020-06-15T16:06:20.373946Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:21.086974Z",
     "start_time": "2020-06-15T16:06:20.731688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      " 2175/26280 [=>............................] - ETA: 25:20 - loss: 596.9106 - mean_absolute_error: 14.0119"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    SliceGenerator(img_dir=train_file_path, slice_size=frames, \n",
    "                    vars_=vars_, proc_type=proc_type,\n",
    "                    pixels_x=pixels_x, pixels_y=pixels_y, debug=False\n",
    "                    ),\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs = epochs,\n",
    "    verbose = 1,\n",
    "    shuffle = False,\n",
    "    callbacks=callbacks,\n",
    "    validation_steps = valid_steps,\n",
    "    validation_data = SliceGenerator(img_dir=valid_file_path, slice_size=frames,\n",
    "                                      vars_=vars_, proc_type=proc_type,\n",
    "                                      pixels_x=pixels_x, pixels_y=pixels_y, debug=False\n",
    "                                      ),\n",
    ")\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "        model = model,\n",
    "        filepath = pwd+model_name+'.h5',\n",
    "        overwrite=True,\n",
    "        include_optimizer=True,\n",
    "        save_format='tf',\n",
    "        signatures=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model history:\")\n",
    "pprint(model_history)\n",
    "pickle.dump(model_history, open(model_dir+model_folder+model_name+\"_hist.pkl\", \"wb\"))\n",
    "print(\"Model history saved to:\",model_dir+model_folder+model_name+\"_hist.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = model_history[\"loss\"]\n",
    "val_loss = model_history[\"val_loss\"]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Number of Epochs\", size=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(model_dir+model_folder+model_name+\"_loss.png\",transparent=True, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually Inspect Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:06:20.106368Z",
     "start_time": "2020-06-15T16:06:19.874866Z"
    }
   },
   "outputs": [],
   "source": [
    "slice_val = SliceGenerator(img_dir=train_file_path, slice_size=frames, vars_=vars_,\n",
    "                     proc_type=proc_type, pixels_x=pixels_x, pixels_y=pixels_y, debug=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T15:56:54.502318Z",
     "start_time": "2020-06-15T15:56:54.472843Z"
    }
   },
   "outputs": [],
   "source": [
    "in_, out_ = next(slice_val)\n",
    "in_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred= model.predict(in_, verbose=1)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
