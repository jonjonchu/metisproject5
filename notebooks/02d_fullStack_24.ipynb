{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM functional encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:02:13.471674Z",
     "start_time": "2020-06-15T16:02:13.436638Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:02:14.243042Z",
     "start_time": "2020-06-15T16:02:14.237371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version:\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dimension variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:02:14.878387Z",
     "start_time": "2020-06-15T16:02:14.873906Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = 24\n",
    "channels = 1\n",
    "pixels_x = 21\n",
    "pixels_y = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Old versions of Tensorflow (<2.2) may not have support for initial_state on ConvLSTM2D cells. \n",
    "Please follow the instructions here: https://stackoverflow.com/questions/50253138/convlstm2d-initial-state-assertion-error to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:02:16.364824Z",
     "start_time": "2020-06-15T16:02:16.352422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_stack_24f_1c_21x_21y\n"
     ]
    }
   ],
   "source": [
    "model_name = 'full_stack_'+str(frames)+'f_'+str(channels)+'c_'+str(pixels_x)+'x_'+str(pixels_y)+'y'\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full stack model:\n",
    "1. split inputs into values (inputA) and targets (inputB)\n",
    "\n",
    "2. Run each through a deep convolutional layer to reduce dimensionality\n",
    "\n",
    "3. run conv'd values through encoder\n",
    "\n",
    "4. pass encoder hidden and cell states to decoder, also pass conv'd targets as decoder input\n",
    "\n",
    "5. get decoder output and run it through deconvolution network to reassemble prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer convA1 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, 24, 1, 21, 21]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ef18a2db1ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBN_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 886\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    887\u001b[0m         if (any(isinstance(x, ragged_tensor.RaggedTensor) for x in input_list)\n\u001b[1;32m    888\u001b[0m             and self._supports_ragged_inputs is False):  # pylint: disable=g-bool-id-comparison\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer convA1 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [None, 24, 1, 21, 21]"
     ]
    }
   ],
   "source": [
    "### Conv Stack Vars\n",
    "input_shape = (channels,pixels_x,pixels_y)\n",
    "weight_decay=1e-5\n",
    "filters=[8, 16, 16]\n",
    "kernel_sizes = [(5,5), (3,3), (3,3)]\n",
    "strides=[(2,2),(1,1),(2,2)]\n",
    "bias_init=0.1\n",
    "output_activation=tf.nn.sigmoid,\n",
    "\n",
    "########### 1. INPUT PARSING ###########\n",
    "\n",
    "inputs = layers.Input(name=\"model_input\",\n",
    "                      shape = (2*frames, channels, pixels_x, pixels_y))\n",
    "\n",
    "inputA, inputB = tf.split(inputs, 2, axis=1, num=None, name='split')\n",
    "\n",
    "inputA = layers.GaussianNoise(0.1)(inputA)\n",
    "\n",
    "########### 2a. CONV A (input t=0) ################\n",
    "\n",
    "name = \"convA\"\n",
    "\n",
    "Conv2D_1 = layers.Conv2D(name=name+\"1\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=filters[0],\n",
    "                         kernel_size=kernel_sizes[0],\n",
    "                         strides=strides[0],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "BN_1 = layers.BatchNormalization(axis=1, name=name+\"1_bn\")\n",
    "Conv2D_2 = layers.Conv2D(name=name+\"2\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=filters[1],\n",
    "                         kernel_size=kernel_sizes[1],\n",
    "                         strides=strides[1],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "BN_2 = layers.BatchNormalization(axis=1, name=name+\"2_bn\")\n",
    "Conv2D_3 = layers.Conv2D(name=name+\"3\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=filters[2],\n",
    "                         kernel_size=kernel_sizes[2],\n",
    "                         strides=strides[2],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "BN_3 = layers.BatchNormalization(axis=1, name=name+\"3_bn\")\n",
    "\n",
    "\n",
    "stack = Conv2D_1(inputA)\n",
    "stack = BN_1(stack)\n",
    "stack = Conv2D_2(stack)\n",
    "stack = BN_2(stack)\n",
    "stack = Conv2D_3(stack)\n",
    "convA_output = BN_3(stack)\n",
    "\n",
    "###### 2b. CONV B (input t=1, aka target) ##############\n",
    "\n",
    "name = \"convB\"\n",
    "\n",
    "Conv2D_1B = layers.Conv2D(name=name+\"1\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=filters[0],\n",
    "                         kernel_size=kernel_sizes[0],\n",
    "                         strides=strides[0],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "BN_1B = layers.BatchNormalization(axis=1, name=name+\"1_bn\")\n",
    "Conv2D_2B = layers.Conv2D(name=name+\"2\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=filters[1],\n",
    "                         kernel_size=kernel_sizes[1],\n",
    "                         strides=strides[1],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "BN_2B = layers.BatchNormalization(axis=1, name=name+\"2_bn\")\n",
    "Conv2D_3B = layers.Conv2D(name=name+\"3\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=filters[2],\n",
    "                         kernel_size=kernel_sizes[2],\n",
    "                         strides=strides[2],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "BN_3B = layers.BatchNormalization(axis=1, name=name+\"3_bn\")\n",
    "\n",
    "\n",
    "stackB = Conv2D_1B(inputB)\n",
    "stackB = BN_1B(stackB)\n",
    "stackB = Conv2D_2(stackB)\n",
    "stackB = BN_2B(stackB)\n",
    "stackB = Conv2D_3B(stackB)\n",
    "convB_output = BN_3(stackB)\n",
    "\n",
    "####################################\n",
    "####### ENCODER-DEC0DER ############\n",
    "\n",
    "########## 3. ENCODER ##############\n",
    "\n",
    "# first time-step\n",
    "i = 0\n",
    "# get input_images and output_images as one tensor\n",
    "encoder_input = tf.expand_dims(convA_output, 0)\n",
    "encoder_cell_1 = layers.ConvLSTM2D(name=\"encoder{}\".format(i+1),\n",
    "                                   filters = filters[-1],\n",
    "                                   kernel_size=(5,5),\n",
    "                                   padding='same',\n",
    "                                   data_format='channels_first',\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "_, state_h, state_c = encoder_cell_1(encoder_input)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "##### 4. DECODER #####\n",
    "\n",
    "decoder_input = tf.expand_dims(convB_output, 0)\n",
    "decoder_cell_1 = layers.ConvLSTM2D(name=\"decoder{}\".format(i+1),\n",
    "                                   filters = filters[-1],\n",
    "                                   kernel_size=(5,5),\n",
    "                                   padding='same',\n",
    "                                   data_format='channels_first',\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "decoder_output, _, _ = decoder_cell_1(decoder_input, initial_state = encoder_states)\n",
    "reshaped_decoder_output = tf.reshape(decoder_output,shape=[-1, filters[-1], 3, 3])\n",
    "\n",
    "\n",
    "#################################\n",
    "###### 5. DECONV STACK ##########\n",
    "\n",
    "name = \"deconv\"\n",
    "rev_filters = filters[::-1]\n",
    "rev_filters = rev_filters[1:] + [channels]\n",
    "rev_kernel_sizes = kernel_sizes[::-1]\n",
    "rev_strides = strides[::-1]\n",
    "\n",
    "deConv2D_1 = layers.Conv2DTranspose(name=name+\"1\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=rev_filters[0],\n",
    "                         kernel_size=rev_kernel_sizes[0],\n",
    "                         strides=rev_strides[0],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "deBN_1 = layers.BatchNormalization(axis=1, name=name+\"1_bn\")\n",
    "deConv2D_2 = layers.Conv2DTranspose(name=name+\"2\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=rev_filters[1],\n",
    "                         kernel_size=rev_kernel_sizes[1],\n",
    "                         strides=rev_strides[1],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "deBN_2 = layers.BatchNormalization(axis=1, name=name+\"2_bn\")\n",
    "deConv2D_3 = layers.Conv2DTranspose(name=name+\"3\",\n",
    "                         data_format='channels_first',\n",
    "                         filters=rev_filters[2],\n",
    "                         kernel_size=rev_kernel_sizes[2],\n",
    "                         strides=rev_strides[2],\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                         activity_regularizer=tf.keras.regularizers.l2(l=weight_decay),\n",
    "                         activation=\"relu\",\n",
    "                         )\n",
    "deBN_3 = layers.BatchNormalization(axis=1, name=name+\"3_bn\")\n",
    "\n",
    "# reshaped = reshaper(decoder_output)\n",
    "stack = deConv2D_1(reshaped_decoder_output)\n",
    "stack = deBN_1(stack)\n",
    "stack = deConv2D_2(stack)\n",
    "stack = deBN_2(stack)\n",
    "stack = deConv2D_3(stack)\n",
    "stack = deBN_3(stack)\n",
    "deconv_output = tf.expand_dims(stack, 0)\n",
    "\n",
    "#######################\n",
    "\n",
    "full_model = tf.keras.Model(name=\"Full_stack\",\n",
    "                       inputs = inputs,\n",
    "                       outputs = deconv_output)\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.compile(loss='KLDivergence',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy', 'mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T16:05:00.478498Z",
     "start_time": "2020-06-15T16:05:00.429464Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(\n",
    "    model = full_model,\n",
    "    filepath = '../models/'+model_name+'.h5',\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format='tf',\n",
    "    signatures=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.Variable(tf.random.uniform([1,1,8,3,3], -1, 1))\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(inputs, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_filters = filters[::-1]\n",
    "rev_filters = rev_filters[1:] + [channels]\n",
    "rev_ksizes = kernel_sizes[::-1]\n",
    "rev_strides = strides[::-1]\n",
    "print(rev_filters)\n",
    "print(rev_ksizes)\n",
    "print(rev_strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
